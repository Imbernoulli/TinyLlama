# TinyLlama 自定义实验指南

本指南说明如何在 TinyLlama 代码库中自定义**激活函数**和**位置编码**，用于实验不同的架构选择。

## 目录
- [激活函数自定义](#激活函数自定义)
- [位置编码自定义](#位置编码自定义)
- [配置示例](#配置示例)
- [常见问题](#常见问题)

---

## 激活函数自定义

### 现有 MLP 类

每种激活函数都有对应的独立 MLP 类：

**传统激活函数**（单投影）：
- `GeluMLP`: GELU 激活函数
- `SiluMLP`: SiLU (Swish) 激活函数
- `ReluMLP`: ReLU 激活函数

**GLU 变体**（双投影 + 门控）：
- `SwiGLUMLP`: SiLU 门控线性单元
- `GeGLUMLP`: GELU 门控线性单元
- `ReGLUMLP`: ReLU 门控线性单元

**遗留类**（用于加载旧模型）：
- `GptNeoxMLP`: GPT-NeoX 风格（GELU）
- `LLaMAMLP`: LLaMA 风格（使用 xformers SwiGLU）

### 如何使用现有激活函数

在配置文件中直接设置 `_mlp_class`:

```python
# 示例：使用 GeGLU 激活函数的 TinyLlama
dict(
    name="tiny_LLaMA_1b_geglu",
    _mlp_class="GeGLUMLP",
    intermediate_size=5632,  # GLU 激活需要显式设置
    # ... 其他配置 ...
)
```

### 如何添加自定义激活函数

#### 步骤 1: 在 `lit_gpt/model.py` 中添加新的 MLP 类

**传统激活示例**：

```python
class MishMLP(nn.Module):
    """MLP with Mish activation."""
    def __init__(self, config: Config) -> None:
        super().__init__()
        self.fc = nn.Linear(config.n_embd, config.intermediate_size, bias=config.bias)
        self.proj = nn.Linear(config.intermediate_size, config.n_embd, bias=config.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.fc(x)
        x = x * torch.tanh(torch.nn.functional.softplus(x))  # Mish
        return self.proj(x)
```

**GLU 变体示例**：

```python
class MishGLUMLP(nn.Module):
    """MLP with Mish-gated GLU."""
    def __init__(self, config: Config) -> None:
        super().__init__()
        self.fc_1 = nn.Linear(config.n_embd, config.intermediate_size, bias=config.bias)
        self.fc_2 = nn.Linear(config.n_embd, config.intermediate_size, bias=config.bias)
        self.proj = nn.Linear(config.intermediate_size, config.n_embd, bias=config.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        gate = self.fc_1(x)
        up = self.fc_2(x)
        gate = gate * torch.tanh(torch.nn.functional.softplus(gate))  # Mish
        return self.proj(gate * up)
```

#### 步骤 2: 更新配置类型定义

在 `lit_gpt/config.py` 中添加类名到 `Literal`:

```python
_mlp_class: Literal["GptNeoxMLP", "LLaMAMLP", "GeluMLP", "SiluMLP", "ReluMLP",
                    "SwiGLUMLP", "GeGLUMLP", "ReGLUMLP", "MishMLP", "MishGLUMLP"] = "GptNeoxMLP"
```

#### 步骤 3: 如果是 GLU 类型，更新 `__post_init__` 检查

在 `lit_gpt/config.py` 的 `__post_init__` 中：

```python
glu_mlp_classes = {"LLaMAMLP", "SwiGLUMLP", "GeGLUMLP", "ReGLUMLP", "MishGLUMLP"}
```

#### 步骤 4: 更新权重初始化

在 `lit_gpt/model.py` 的 `_init_weights` 中：

```python
if (name == "proj.weight" and isinstance(module, (..., MishMLP, MishGLUMLP, ...))) or ...
```

#### 步骤 5: 在配置中使用

```python
dict(
    name="my_experiment",
    _mlp_class="MishMLP",
    # ...
)
```

### 架构对比

**传统激活** (如 GELU)：
```
输入 → fc (线性层) → 激活函数 → proj (线性层) → 输出
```

**GLU 激活** (如 SwiGLU)：
```
       ┌─→ fc_1 (gate) ──→ 激活函数 ──┐
输入 ──┤                              ├─→ 乘法 → proj → 输出
       └─→ fc_2 (up) ────────────────┘
```

### 参数量对比

- **传统激活**: `2 × n_embd × intermediate_size` 参数
- **GLU 激活**: `3 × n_embd × intermediate_size` 参数（多 50%）

因此从 GLU 切换到传统激活时，如果要保持参数量相近，可以增大 `intermediate_size`。

---

## 位置编码自定义

### 现有位置编码

位置编码在 `lit_gpt/positional_encodings.py` 中定义：

**基础位置编码**：
- `rope`: 标准旋转位置编码（RoPE）
- `none`: 无位置编码（NoPE）

**RoPE 变体**（用于长度外推）：
- `dynamic_ntk`: 动态 NTK-Aware RoPE，根据序列长度自动调整
- `yarn`: YaRN (Yet another RoPE extensioN)，使用温度缩放和频率分段
- `linear_scaled`: 线性缩放 RoPE（位置插值 PI），通过 `condense_ratio` 实现

### 如何使用现有位置编码

在配置中设置 `_pos_encoding`:

```python
# 示例 1：标准 RoPE
dict(
    _pos_encoding="rope",
    rope_base=10000,  # 可调整基频
    rotary_percentage=1.0,  # RoPE 应用的维度比例
    # ...
)

# 示例 2：动态 NTK RoPE（推荐用于长上下文）
dict(
    _pos_encoding="dynamic_ntk",
    rope_base=10000,  # 基频（会自动调整）
    block_size=8192,  # 目标上下文长度
    # 自动为 > 2048 的序列应用 NTK 缩放
)

# 示例 3：YaRN（最先进的长度外推）
dict(
    _pos_encoding="yarn",
    rope_base=10000,
    block_size=16384,  # 支持超长上下文
    # 使用温度缩放，高频和低频分别处理
)

# 示例 4：线性缩放（简单但有效）
dict(
    _pos_encoding="linear_scaled",
    condense_ratio=4,  # 4 倍压缩（支持 4 倍上下文）
    block_size=8192,   # 4 × 2048
    # ...
)

# 示例 5：不使用位置编码
dict(
    _pos_encoding="none",
    # ...
)
```

### 如何添加自定义位置编码

#### 步骤 1: 编辑 `lit_gpt/positional_encodings.py`

添加两个函数：`build_cache` 和 `apply`:

```python
# 示例：添加 ALiBi 位置编码
def build_alibi_cache(
    seq_len: int,
    n_elem: int,  # 参数保持一致，但可以忽略不用的
    dtype: torch.dtype,
    device: torch.device,
    base: int = 10000,
    condense_ratio: int = 1,
) -> RoPECache:
    """ALiBi 不需要 cos/sin 表，返回虚拟缓存"""
    # 返回占位符张量
    dummy = torch.zeros(seq_len, max(n_elem // 2, 1), device=device, dtype=dtype)
    return dummy, dummy


def apply_alibi(
    q: torch.Tensor,
    k: torch.Tensor,
    cos: torch.Tensor,  # 不使用，但保持接口一致
    sin: torch.Tensor
) -> Tuple[torch.Tensor, torch.Tensor]:
    """ALiBi 不修改 q, k，而是在注意力分数上加偏置"""
    # 对于 ALiBi，q 和 k 不变
    # 真正的偏置需要在 CausalSelfAttention.scaled_dot_product_attention 中添加
    return q, k
```

#### 步骤 2: 注册到 `POS_ENCODING_REGISTRY`

```python
POS_ENCODING_REGISTRY = {
    "rope": {"build_cache": build_rope_cache, "apply": apply_rope},
    "none": {"build_cache": build_none_cache, "apply": apply_none},
    "alibi": {"build_cache": build_alibi_cache, "apply": apply_alibi},  # 添加
}
```

#### 步骤 3: 在配置中使用

```python
dict(
    _pos_encoding="alibi",
    # ...
)
```

### RoPE 变体对比

| 方法 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| **标准 RoPE** | 简单高效 | 外推能力有限 | 标准训练长度内 |
| **Dynamic NTK** | 自动调整，无需手动计算 | 可能不如 YaRN 精确 | 通用长上下文，推理时长度可变 |
| **YaRN** | 最佳外推性能 | 稍复杂 | 超长上下文（16k+），追求最佳性能 |
| **Linear Scaled** | 极简，易理解 | 性能略低于 NTK 方法 | 简单实验，固定倍数扩展 |

### RoPE 参数调整

#### 方法 1: 动态 NTK（推荐）

使用 `_pos_encoding="dynamic_ntk"`，无需手动计算：

```python
dict(
    _pos_encoding="dynamic_ntk",
    block_size=8192,  # 目标上下文长度
    rope_base=10000,  # 保持默认即可
    # 自动为 seq_len > 2048 应用 NTK 缩放
)
```

#### 方法 2: 手动 NTK 缩放

计算新的 `rope_base` 并使用标准 RoPE：

```python
# 计算公式
# new_base = 10000 × (target_len / train_len) ^ (dim / (dim - 2))
#
# 示例：2048 -> 8192 (4倍)，dim=128
# new_base = 10000 × 4^(128/126) ≈ 40317

dict(
    _pos_encoding="rope",
    block_size=8192,
    rope_base=40317,  # 手动计算的缩放基频
)
```

#### 方法 3: YaRN（最佳性能）

```python
dict(
    _pos_encoding="yarn",
    block_size=16384,  # 支持超长上下文
    rope_base=10000,
    # YaRN 自动应用温度缩放和频率分段
)
```

#### 方法 4: 线性缩放

通过 `condense_ratio` 压缩位置索引：

```python
dict(
    _pos_encoding="linear_scaled",  # 或使用 "rope"
    condense_ratio=4,  # 4 倍压缩
    block_size=8192,   # 4 × 2048
)
```

#### RoPE 应用比例

`rotary_percentage` 控制每个注意力头中应用 RoPE 的维度比例：

```python
rotary_percentage=1.0  # 所有维度（LLaMA 风格）
rotary_percentage=0.25  # 25% 维度（GPT-NeoX 风格）
rotary_percentage=0.0  # 等价于 NoPE（不推荐，直接用 _pos_encoding="none"）
```

---

## 配置示例

### 示例 1: TinyLlama 使用 GeGLU 激活

```python
dict(
    org="StatNLP-research",
    name="tiny_LLaMA_1b_geglu",
    block_size=2048,
    vocab_size=32000,
    padding_multiple=64,
    n_layer=22,
    n_head=32,
    n_embd=2048,
    rotary_percentage=1.0,
    parallel_residual=False,
    bias=False,
    _norm_class="FusedRMSNorm",
    norm_eps=1e-5,
    _mlp_class="GeGLUMLP",      # 使用 GeGLU MLP 类
    intermediate_size=5632,     # 必须显式设置
    n_query_groups=4,
    _pos_encoding="rope",       # 标准 RoPE
    rope_base=10000,
)
```

### 示例 2: TinyLlama 使用传统 GELU（非 GLU）

```python
dict(
    org="StatNLP-research",
    name="tiny_LLaMA_1b_gelu",
    # ... 基本配置同上 ...
    _mlp_class="GeluMLP",       # 使用 GELU MLP 类
    intermediate_size=8448,     # 可以设置更大（因为 GELU 参数少 33%）
    # 或不设置 intermediate_size，默认 4 × n_embd = 8192
)
```

### 示例 3: 无位置编码实验

```python
dict(
    org="StatNLP-research",
    name="tiny_LLaMA_1b_nope",
    # ... 基本配置 ...
    _pos_encoding="none",       # 不使用位置编码
    rotary_percentage=0.0,      # 设为 0（虽然 none 编码下会被忽略）
)
```

### 示例 4: 长上下文实验（多种方法对比）

```python
# 方法 1: Dynamic NTK（推荐，自动调整）
dict(
    org="StatNLP-research",
    name="tiny_LLaMA_1b_long_dynamic",
    block_size=8192,            # 4 倍扩展
    _pos_encoding="dynamic_ntk",
    rope_base=10000,            # 无需调整
    # ... 其他配置 ...
)

# 方法 2: YaRN（最佳性能）
dict(
    org="StatNLP-research",
    name="tiny_LLaMA_1b_long_yarn",
    block_size=16384,           # 8 倍扩展
    _pos_encoding="yarn",
    rope_base=10000,
    # ... 其他配置 ...
)

# 方法 3: 线性缩放（最简单）
dict(
    org="StatNLP-research",
    name="tiny_LLaMA_1b_long_linear",
    block_size=8192,
    _pos_encoding="linear_scaled",
    condense_ratio=4,           # 4 倍压缩
    # ... 其他配置 ...
)

# 方法 4: 手动 NTK 缩放
dict(
    org="StatNLP-research",
    name="tiny_LLaMA_1b_long_manual",
    block_size=8192,
    _pos_encoding="rope",
    rope_base=40317,            # 手动计算：10000 × 4^(128/126)
    # ... 其他配置 ...
)
```

---

## 常见问题

### 1. 如何在现有检查点上继续训练但改变激活函数？

**答**: 激活函数改变会导致模型结构变化，无法直接加载检查点。建议：
- 从头训练新配置
- 或者实现权重转换脚本（如果新旧结构兼容）

### 2. GLU 激活的 `intermediate_size` 应该设置多少？

**答**:
- **保持参数量**: GLU 比传统多 50% 参数。如果传统 MLP 用 `intermediate_size = 4 × n_embd`，GLU 应该用 `intermediate_size ≈ 2.67 × n_embd` 来匹配参数量
- **LLaMA 风格**: LLaMA-2 7B 使用 `intermediate_size = 11008` (约 2.7 倍 n_embd=4096)，TinyLlama 使用 `5632` (约 2.7 倍 n_embd=2048)

### 3. 修改位置编码后需要重新训练吗？

**答**: 是的，位置编码是模型的核心组成部分，修改后必须重新训练。但可以：
- 保持 `_pos_encoding="rope"` 不变
- 仅调整 `rope_base` 或 `rotary_percentage` 参数做消融实验

### 4. 旧配置（`_mlp_class="GptNeoxMLP"` 或 `"LLaMAMLP"`）还能用吗？

**答**: 可以！旧的 `GptNeoxMLP` 和 `LLaMAMLP` 类仍然保留，用于加载现有检查点。新实验直接使用新的 MLP 类（如 `GeluMLP`, `SwiGLUMLP` 等）。

### 5. 如何验证自定义激活是否生效？

**答**: 在训练脚本中添加打印：

```python
config = Config.from_name("my_experiment")
model = GPT(config)

# 查看 MLP 层
block = model.transformer.h[0]
print(f"MLP class: {type(block.mlp).__name__}")

# 查看层参数
for name, param in block.mlp.named_parameters():
    print(f"  {name}: {param.shape}")
```

### 6. 哪个 RoPE 变体最好？

**答**: 取决于场景：

- **标准训练（≤ 2048）**: 使用 `rope`，简单高效
- **长上下文推理（可变长度）**: 使用 `dynamic_ntk`，自动适应不同长度
- **超长上下文（16k+）**: 使用 `yarn`，最佳外推性能
- **固定倍数扩展**: 使用 `linear_scaled` + `condense_ratio`，最简单
- **学术对比实验**: 同时测试多个变体，选择最优

推荐流程：`dynamic_ntk`（通用） → `yarn`（追求极致） → `linear_scaled`（快速原型）

### 7. 位置编码缓存如何工作？

**答**:
- 缓存在 `GPT.build_rope_cache()` 中构建一次
- 存储为 `(cos, sin)` 张量元组
- 每层的注意力共享同一个缓存
- 推理时根据 `input_pos` 索引缓存

---

## 文件修改总结

### 新增文件
- `lit_gpt/positional_encodings.py` - 位置编码注册表

### 修改文件
- `lit_gpt/model.py` - 添加新的 MLP 类（`GeluMLP`, `SiluMLP`, `ReluMLP`, `SwiGLUMLP`, `GeGLUMLP`, `ReGLUMLP`）
- `lit_gpt/config.py` - 添加 `_pos_encoding`, `rope_base` 字段，扩展 `_mlp_class` 支持的类型

### 关键类和方法
- MLP 类（[model.py:323-405](lit_gpt/model.py#L323-L405)）- 各种激活函数的 MLP 实现
- `GPT.build_rope_cache()`（[model.py:119-128](lit_gpt/model.py#L119-L128)）- 通过配置构建位置编码缓存
- `CausalSelfAttention.forward()`（[model.py:200-268](lit_gpt/model.py#L200-L268)）- 应用位置编码到 q, k
- `POS_ENCODING_REGISTRY`（[positional_encodings.py:94-97](lit_gpt/positional_encodings.py#L94-L97)）- 位置编码注册表

---

## 快速开始

### 1. 创建实验配置

在 `lit_gpt/config.py` 的 `configs` 列表末尾添加：

```python
# 你的实验配置
my_experiments = [
    dict(
        org="my-org",
        name="tinyllama_experiment_1",
        block_size=2048,
        vocab_size=32000,
        padding_multiple=64,
        n_layer=22,
        n_head=32,
        n_embd=2048,
        rotary_percentage=1.0,
        parallel_residual=False,
        bias=False,
        _norm_class="FusedRMSNorm",
        norm_eps=1e-5,
        _mlp_class="SwiGLUMLP",    # 选择 MLP 类（即激活函数）
        _pos_encoding="rope",      # 选择位置编码
        rope_base=10000,
        intermediate_size=5632,
        n_query_groups=4,
    ),
]
configs.extend(my_experiments)
```

### 2. 使用配置训练

```bash
# 预训练
python pretrain/tinyllama.py --config_name tinyllama_experiment_1

# 或直接传参覆盖
python pretrain/tinyllama.py \
    --config_name tiny_LLaMA_1b \
    --_mlp_class GeGLUMLP \
    --_pos_encoding rope
```

### 3. 对比不同配置

创建多个配置做消融研究：

```python
ablations = [
    dict(name="ablation_swiglu", _mlp_class="SwiGLUMLP", intermediate_size=5632, ...),
    dict(name="ablation_geglu", _mlp_class="GeGLUMLP", intermediate_size=5632, ...),
    dict(name="ablation_gelu", _mlp_class="GeluMLP", intermediate_size=8192, ...),
]
```

---

## 进阶：扩展性

### 添加 Mish 激活函数

参考上面 "如何添加自定义激活函数" 的步骤 1，创建 `MishMLP` 类即可。

### 添加 ALiBi 位置编码（需修改注意力）

ALiBi 需要在注意力分数上添加偏置，需要修改 `CausalSelfAttention.scaled_dot_product_attention()`：

```python
def scaled_dot_product_attention(self, q, k, v, mask=None):
    # ... 现有代码 ...

    # 如果使用 ALiBi
    if self.config._pos_encoding == "alibi":
        alibi_bias = self._get_alibi_bias(...)  # 实现 ALiBi 偏置
        # 添加到注意力分数
```

这需要更深入的修改，建议先熟悉 RoPE 和 NoPE 的实现。

---

## 参考资料

- **RoPE 论文**: [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
- **GLU 变体**: [GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202)
- **NTK-Aware Scaling**: [Reddit Discussion](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/)
- **ALiBi**: [Train Short, Test Long: Attention with Linear Biases](https://arxiv.org/abs/2108.12409)

---

## 更新日志

- **2024-XX-XX**: 初始版本，支持激活函数和位置编码自定义
